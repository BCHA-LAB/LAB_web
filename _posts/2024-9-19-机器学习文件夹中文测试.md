# 第一讲 8.18
## 名词解释
 
目前是Structured Learning,创造有结构的东西,比如一篇文章,一个图片.
机器学习在找一个函数
## 机器学习三个步骤
1.找出函数需要多少参数
![[Pasted image 20240818145126.png]]
2.计算loss,预测结果和真正结果的差距.
e表示每个真实数据和预测数据的差距
![[Pasted image 20240818150320.png]]

3.最小化L
![[Pasted image 20240818155056.png]]
_____
总结:设计函数的参数,计算不同参数配比下的loss,寻找最小loss的参数组合.
**Linear Model**线性模型,一系列features配比不同权重求和.
_____________
# 机器学习第一讲第二课

8.19

## 思考：如何写出一个更复杂的fuction？

假设目标函数是一个Piecewise Liner Curve，可以使用set of折线叠加来拟合。

如果是曲线，可以在曲线上取足够多的点，就可以变成锯齿状函数。

### 如何表现一个折线分量？（Hard Sigmoid）

可以用y=c sigmoid(b+wx1)（S形函数）

w：拟合斜率 b：拟合零点 c：拟合高度

## 向量表示方法，**r**=**b**+**w*****x**

---

总结：更高级的表达方式是：把一个曲线表现为折线，将折线用叠加的方法分解为a set of sigmoid函数,sigmoid的参数可以用矩阵表示。 图解如下：

---

## 计算Loss

图解如下：

---

## Optimization

选择一个初始值，遍历向量参数计算gradient梯度，计算局部最优参数配比。 当L过大时，可以把L分为N个batch，每次可以计算不同batch的局部最优参数。当全部batch的局部最优参数update之后，称之为epoch（L全部batch update一次）。

---

Hard sigmoid也可以用两个Rectified Linear Unit（ReLU）叠加。

图解如下：

将sigmoid变成hard sigmoid会增加一倍参数。

在今天的例子中，sigmoid与max函数均称为Activation Function。

---

## 做多次相似的训练：计算出**a**之后，把**a**作为下一层的输入，再计算出**a'**

给出一个**fancy name**：**Neuron Network** 每一层称为**hidden layer**

图解如下：

并不是训练越深越好：overfitted过拟合
____
## 第一讲 补充部分
### backpropagation
![[Pasted image 20240820111246.png]]
利用嵌套函数的性质,从最外层往里面算偏微分可以提升效率,避免重复计算
![[Pasted image 20240820111551.png]]

____
总结:forward Pass计算Loss,Backward Pass计算optimal
![[Pasted image 20240820111820.png]]
### 分类算法
Perception与SVM现在先不讲
![[Pasted image 20240820130040.png]]
三部分的思路如上
贝叶斯公式:
![[Pasted image 20240820132036.png]]
可以计算在给定x的条件下,它属于哪个类别的几率是多少,与挑选出x的几率是多少
![[Pasted image 20240820132422.png]]
**Denerative Model**
![[Pasted image 20240820133203.png]]
假设几率函数是一个二元正态分布
如何找上面这个式子的参数?
**Maximun Likelihood**
![[Pasted image 20240820133409.png]]
由此得出Loss函数:得到全部sample概率最大的点
![[Pasted image 20240820133523.png]]
正态分布数学上的最值公式
![[Pasted image 20240820134015.png]]
由此带入概率函数
![[Pasted image 20240820134249.png]]
分类概率依据
![[Pasted image 20240820134312.png]]
为了减少参数避免过拟合,可以公用一个Σ
![[Pasted image 20240820135107.png]]
_____
## 总结 二元分类的概率模型
![[Pasted image 20240820135525.png]]
![[Pasted image 20240820140632.png]]
在相同协方差选择下,可以化简成类似一个layer的线代表示
## 第二讲 第一课
8.19
![[Pasted image 20240819170420.png]]决策树如上
_____
限制模型不要过于弹性(最优解范围不要太大)
![[Pasted image 20240819170745.png]]
,当然也不要too restrict,会产生更多的modal bias.
一张图解释modal bias与over fit
![[Pasted image 20240819171000.png]]
_________
当样本过少时,可以使用N-foldCross Vaidation
![[Pasted image 20240819172416.png]]

此外,optimization可以因为偏导函数为0loss不减少
![[Pasted image 20240819213547.png]]

这种点称为critical point
`local minima`no way to go
但是saddle point检测到之后可以escape:**如何判断?**
![[Pasted image 20240819213946.png]]
sitar做三项泰勒展开(线代形式)
,当一阶导为0时,用二阶导来判断(最大值定理)
![[Pasted image 20240819214420.png]]
这里使用了正交性矩阵的方法简化计算
# 第二讲 第二课
8.20
为什么分batch,什么是epoch?
![[Pasted image 20240820141542.png]]
选择合适的batch size 的重要性
![[Pasted image 20240820141834.png]]
由于GPU可以进行平行运算,所以在一定范围内batch size越大时间不一定会爆炸增长
![[Pasted image 20240820142505.png]]
选择合适的batch size可以提高效率
 ![[Pasted image 20240820143242.png]]
 对于optimal来说,反而是小的batch(noisy)的accuracy好
 ![[Pasted image 20240820143458.png]]
 多组数据求梯度下降
 ![[Pasted image 20240820143758.png]]
 flat minima的testing loss可能会更好(他的容错率更好)
 ![[Pasted image 20240820144005.png]]
  **vanilla Gradient Descent**
  ![[Pasted image 20240820144631.png]]
  只考虑梯度下降方向更新参数
  Momentum:梯度＋上一次位移
  ![[Pasted image 20240820144804.png]]
  ![[Pasted image 20240820145221.png]]
  ____
  总结:batch需要选择合适的参数,momentum给解决critical point提供了一种思路
  ___
  ## 第二讲 第三节
  8.20
  **Adaptive Learning Rate**
  ![[Pasted image 20240820150051.png]]
  loss不变的时候,gradient可以还会变化,就像卡在山谷里面了,左右跳来跳去loss不变gradient在变
  重点:客制化learning rate
  对于不同参数,不同梯度,都给一个独立的学习速率
  ![[Pasted image 20240820151220.png]]
  Root Mean Square平方平均数
  Used in Adagrad
  ![[Pasted image 20240820151711.png]]
  这种方法存在一些问题,因为同一个参数的learning rate也应该改变
  上述方法只根据梯度更新LR
  提出新方法:**PMSProp**,σ给内容权重而不是取平均
  ![[Pasted image 20240820152238.png]]
  
  ![[Pasted image 20240820152501.png]]
  我们可以调大α,让最近改变的权重变大,这样update函数更具有"及时性"
  ADAM算法![[Pasted image 20240820152609.png]]
  考虑"坡度","惯性",提高找到critical point的成功率
  继续优化,learning rate Scheduling
  ![[Pasted image 20240820155828.png]]
  时间过得越久,LR补偿越小(让LR一直变小)
  还有一种假设是Warm Up,LR先变大后变小
  ![[Pasted image 20240820160008.png]]
  区别Momentun与σ:Moment对全部梯度求和,是有正负的
  σ只对大小做了RMS或者PMSProp
  ![[Pasted image 20240820161252.png]]
# CNN补充:Spatial Transformer Layer,STL
CNN不能很好的识别同一物体在图片中移动,缩放,旋转
![[Pasted image 20240821111233.png]]
同理,可以解决旋转,缩放
![[Pasted image 20240821111420.png]]
如何用矩阵运算表现放大与缩小,位移?
![[Pasted image 20240821111623.png]]
![[Pasted image 20240821111708.png]]
旋转,缩放,位移均可以使用六个参数的矩阵乘法来表示
NN这个神经网络只需要生成能对应图像操作的参数就可以了
![[Pasted image 20240821111935.png]]
但是这个模型不能用梯度下降优化,因为输入改变一点点,对应的位置是不变的,即gradient\==0
解决方法:**加权平均**
![[Pasted image 20240821112733.png]]
论文图解如下:
![[Pasted image 20240821113026.png]]
STL可以加多层,也可以在同一层加多个
# Self-attention 自注意力机制
![[Pasted image 20240821124355.png]]
处理动态vector set
分词处理方式两种
![[Pasted image 20240821124552.png]]
声音讯号处理方法
![[Pasted image 20240821124756.png]]
Graph也是set of vector
那输出应该是什么样子?
有可能是labels一一对应 如postagging词性分类,语音辨识\
![[Pasted image 20240821125223.png]]
也可能整个sequence输出一个label
![[Pasted image 20240821125402.png]]
也有种可能是不确定个输出,比如翻译与语音辨识
![[Pasted image 20240821125435.png]]
## Sequence Labeling
把每一个向量输入到Fully-connected网络中,一一对应输出
缺陷是独立考虑不能理解上下文
引入Windows概念,每个输入都要考虑window里面的上下文
![[Pasted image 20240821125807.png]]
依然有问题,万一不止考虑上下文,还要考虑sequence,难度是sequence长度是一个动态的概念
## Self-attention
![[Pasted image 20240821125935.png]]
self-attention考虑了整个句子,再来决定输出结果
同样的,self-attention也可以叠加
![[Pasted image 20240821130055.png]]
以生成一层一个作为例子
首先要找向量之间的关联性
![[Pasted image 20240821130441.png]]
如何计算`α`?
常见的有dot-product,additve
![[Pasted image 20240821130615.png]]
接下来使用dot-product,这个是2022年最常用的方法,注意,归一化不一定一定需要使用softmax
![[Pasted image 20240821130928.png]]
α又叫attention scores
![[Pasted image 20240821131146.png]]
同理可以计算所有b-vector
接下来从矩阵乘法来表示
![[Pasted image 20240821132100.png]]
![[Pasted image 20240821132507.png]]
![[Pasted image 20240821132723.png]]
____
总结,以上步骤的全部流程用矩阵乘法表现
![[Pasted image 20240821132911.png]]
优化:所有评价指标可以有多个(也作为一种矩阵)
### Multi-head
![[Pasted image 20240821133308.png]]
同样的,可以得出其他指标,可变成下一个神经网络的输入
![[Pasted image 20240821133433.png]]

以上过程存在一个问题,没有考虑到位置对注意力分数的影响,在处理过程丢失了position information
引入位置编码positional encoding
![[Pasted image 20240821133954.png]]
positional encoding的算法举例
![[Pasted image 20240821134118.png]]
self-attention对于语言类的作用很显著
存在问题:注意力矩阵过大,计算体积过大,又回归类似Windows的方法
![[Pasted image 20240821134358.png]]
同理,在影像处理领域,不但可以把图片看成tensor,也可以看成set of vector\[每一个pix可以看作一个RGB的1×3的向量],用注意力机制做
![[Pasted image 20240821134622.png]]
### CNN可以看作一种简化版的自注意力机制
![[Pasted image 20240821134840.png]]
 由数学可以证明CNN中的receptive field是通过注意力分数找出来的
![[Pasted image 20240821135118.png]]
同样,弹性更大的自注意力机制需要更大的N来避免overfitted
![[Pasted image 20240821135432.png]]
#### 介绍:什么是Recurrent Neural Network(RNN)
![[Pasted image 20240821135917.png]]
在连通图中,我们不需要用注意力矩阵来找向量之间是否又相联系,所以只需要计算邻接矩阵需要的注意力机制
![[Pasted image 20240821140221.png]]
**GNN**比较复杂,水很深
# 第五讲 Transformer
8.21
机器决定输出的数量与长度
![[Pasted image 20240821141252.png]]

![[Pasted image 20240821143103.png]]
文法剖析可以做出类似seq2seq的效果
![[Pasted image 20240821145456.png]]
seq2seq用多label分类
![[Pasted image 20240821145529.png]]
还可以处理多个体识别
流程图
![[Pasted image 20240821145703.png]]
## Encoder架构
输入向量,输出向量
![[Pasted image 20240821145742.png]]
再transformer中用的是自注意力机制
![[Pasted image 20240821145854.png]]
再transformer中,再自注意力之后还又residual connection
![[Pasted image 20240821150224.png]]

![[Pasted image 20240821150406.png]]












